1.过拟合，欠拟合

~~~mark
   欠拟合（Underfitting），即模型没有学到数据内在关系，如下图左一所示，产生分类面不能很好的区分X和O两类数据；产生的深层原因，就是模型假设空间太小或者模型假设空间偏离。
   过拟合（Overfitting），即模型过渡拟合了训练数据的内在关系，如下图右一所示，产生分类面过好地区分X和O两类数据，而真实分类面可能并不是这样，以至于在非训练数据上表现不好；产生的深层原因，是巨大的模型假设空间与稀疏的数据之间的矛盾。
~~~

2.正则化

~~~markdown
   正则化等价于结构风险最小化，其是通过在经验风险项后加上表示模型复杂度的正则化项或惩罚项，达到选择经验风险和模型复杂度都较小的模型目的。
   1.经验风险：
   机器学习中的风险是指模型与真实解之间的误差的积累，经验风险是指使用训练出来的模型进行预测或者分类，存在多大的误差，可以简单理解为训练误差，经验风险最小化即为训练误差最小。
   2.结构风险：
   结构风险定义为经验风险与置信风险(置信是指可信程度)的和，置信风险越大，模型推广能力越差。可以简单认为结构风险是经验风险后面多加了一项表示模型复杂度的函数项，从而可以同时控制模型训练误差和测试误差，结构风险最小化即为在保证模型分类精度(经验风险)的同时，降低模型复杂度，提高泛化能力。
   
~~~

3.留一法

~~~markdown
   留一法，简单来说就是假设有 N 个样本，将每一个样本作为测试样本，其它 N-1 个样本作为训练样本。这样得到 N 个分类器，N 个测试结果。用这 N个结果的平均值来衡量模型的性能。
   线性回归模型中误差的含义是：除X和Y线性关系之外的随机因素对Y的影响
   线性回归均方误差：https://img-blog.csdnimg.cn/20190329171247979.png
~~~

4.**R-square**

​		利用数据拟合一个模型,模型肯定是存在误差的，回归方程对观测值拟合的怎么样，就叫做拟合优度，这里的R-square，就是度量拟合优度的一个统计量，即常说的r方，它叫做可决系数，它的计算方法为

![img](https://cdn.applysquare.net/storage/tmp/qa/SPt2Al6uO/SQXQ2pjRQ.png)

  		分子部分表示真实值与预测值的平方差之和，类似于均方差 MSE；分母部分表示真实值与均值的平方差之和，类似于方差 Var。

​			R-square即可决系数，反映模型对样本数据的拟合程度。值越大，拟合效果越好。它能否反映总体数据的变化情况，要对模型进行检验，R范围是[0,1]。

​			如果使用校正决定系数（Adjusted R-Square）：

![img](http://5b0988e595225.cdn.sohucs.com/images/20180723/4a00a92694f84f6eb1424530bb2f2881.jpeg)

其中，n 是样本数量，p 是特征数量。Adjusted R-Square 抵消样本数量对 R-Square的影响，做到了真正的 0~1，越大越好。

5.残差

~~~mark
1.	残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。
2.	线性回归分析中，目标是残差最小化。残差平方和是关于参数的函数，为了求残差极小值，令残差关于参数的偏导数为零，会得到残差和为零，即残差均值为零。

~~~

6.

~~~mark
	同方差，是为了保证回归参数估计量具有良好的统计性质，经典线性回归模型的一个重要假定：总体回归函数中的随机误差项满足同方差性，即它们都有相同的方差。如果这一假定不满足，		即：随机误差项具有不同的方差，则称线性回归模型存在异方差性。
	通常来说，奇异值的出现会导致异方差性增大。


~~~

7.相关系数

​		相关系数反映了不同变量之间线性相关程度，一般用 r 表示。



![img](https://file.ai100.com.cn/files/sogou-articles/original/f1cb2c70-45ae-4d29-aee7-07ac9cc1ebf6/640.png)



​		其中，Cov(X,Y) 为 X 与 Y 的协方差，Var[X] 为 X 的方差，Var[Y] 为 Y 的方差。r 取值范围在 [-1,1] 之间，r 越大表示相关程度越高。

6.

~~~mark
线性回归一般用于实数预测，逻辑回归一般用于分类问题。
~~~

7.

​		Lasso 回归类似于线性回归，只不过它在线性回归的基础上，增加了一个对所有参数的数值大小约束，如下所示：



![img](https://file.ai100.com.cn/files/sogou-articles/original/5d68c0e5-780b-4362-8f66-0e08e79273f2/640.png)



其中，t 为正则化参数。Lasso 回归其实就是在普通线性回归的损失函数的基础上增加了个 β 的约束。那么 β 的约束为什么要使用这种形式，而不使用 β 的平方约束呢？原因就在于第一范数的约束下，一部分回归系数刚好可以被约束为 0。这样的话，就达到了特征选择的效果。

8.

~~~mark
Lasso 回归会让一部分回归系数刚好可以被约束为 0，起到特征选择的效果。
Ridge 回归又称岭回归，它是普通线性回归加上 L2 正则项，用来防止训练过程中出现的过拟合。
~~~

如果使用校正决定系数（Adjusted R-Squared）：



![img](https://file.ai100.com.cn/files/sogou-articles/original/d8103066-fe5c-457d-a5b5-6b103d00247b/640.jpeg)



其中，n 是样本数量，p 是特征数量。Adjusted R-Squared 抵消样本数量对 R-Squared 的影响，做到了真正的 0~1，越大越好。

增加一个特征变量，如果这个特征有意义，Adjusted R-Square 就会增大，若这个特征是冗余特征，Adjusted R-Squared 就会减小。

9.

RMSE 指的是均方根误差：



![img](https://file.ai100.com.cn/files/sogou-articles/original/cf58d955-304a-47c2-b2cc-f9b691eb6430/640.png)



MSE 指的是均方误差：



![img](https://file.ai100.com.cn/files/sogou-articles/original/4c81c092-4a9d-4a0a-80ee-1963facd990e/640.png)



MAE 指的是评价绝对误差：



![img](https://file.ai100.com.cn/files/sogou-articles/original/4a0ea756-9bfd-430a-b12d-e2da21c77344/640.png)

SSE 是平方误差之和（Sum of Squared Error）

9.

​		如果两个变量相互独立，那么相关系数 r 一定为 0，如果相关系数 r=0，则不一定相互独立。相关系数 r=0 只能说明两个变量之间不存在线性关系，仍然可能存在非线性关系。

10.

   	异常值（Outlier）指样本中的个别值，其数值明显偏离它（或他们）所属样本的其余观测值，也称异常数据，离群值。目前人们对异常值的判别与剔除主要采用物理判别法和统计判别法两种方法。

​		物理判别法就是根据人们对客观事物已有的认识，判别由于外界干扰、人为误差等原因造成实测数据值偏离正常结果，在实验过程中随时判断，随时剔除。

​		统计判别法是给定一个置信概率，并确定一个置信限，凡超过此限的误差，就认为它不属于随机误差范围，将其视为异常值剔除。当物理识别不易判断时，一般采用统计识别法。  